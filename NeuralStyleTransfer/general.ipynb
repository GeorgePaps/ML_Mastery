{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc6c0fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import os\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9016bab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and save image function\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def load_image(path, target_size = None):\n",
    "    image = Image.open(path).convert('RGB')\n",
    "    \n",
    "    if target_size is not None:\n",
    "        # Image.LANCZOS is a high-quality down-sampling filter \n",
    "        # (also known as \"Lanczos resampling\")\n",
    "        # It preserves detail better than simpler filters like NEAREST, BILINEAR\n",
    "        image = image.resize((target_size, target_size), Image.LANCZOS)\n",
    "\n",
    "    # Pixel values from range 0-255 to range 0.0-0.1\n",
    "    # axes are reordered from Pillow's (H,W,C) to (C,H,W)\n",
    "    # which is the Pytorch expected\n",
    "    loader = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "    # inserts a new dimension at index 0,\n",
    "    # turning the tensor into (1,C,H,W)\n",
    "    # the leading 1 represents the batch size - \n",
    "    # even if you load a single image, most PyTorch models expect a batch dimension\n",
    "    tensor = loader(image).unsqueeze(0) # shape: (1, C, H, W)\n",
    "    return tensor.to(device, torch.float)\n",
    "\n",
    "def save_image(tensor, path):\n",
    "    tensor = tensor.detach().cpu().squeeze(0).clamp(0,1)\n",
    "    unloader = transforms.ToPILImage()\n",
    "    image = unloader(tensor)\n",
    "    image.save(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d336d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"Cezanne/Cezanne1.jpg\"\n",
    "\n",
    "tensorA = load_image(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2eaac6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VGG & feature extraction \n",
    "\n",
    "# load VGG-19 with the canonical ImageNet weights\n",
    "vgg_weights = models.VGG19_Weights.IMAGENET1K_V1\n",
    "\n",
    "# with the feature we keep only the convolutional and pooling layers\n",
    "# the eval is used to set the model in inference mode\n",
    "# meaning that layers like dropout or batchnorm will work in inference mode \n",
    "# and they will not introduce randomness in the computations\n",
    "cnn = models.vgg19(weights = vgg_weights).features.to(device).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c6d87d66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'IMAGENET1K_V1': VGG19_Weights.IMAGENET1K_V1, 'DEFAULT': VGG19_Weights.IMAGENET1K_V1}\n"
     ]
    }
   ],
   "source": [
    "print(models.VGG19_Weights.__members__)   # shows all available weight keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60bc3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Normalization used by the pretrained VGG models\n",
    "# RGB values for the training set\n",
    "cnn_normalization_mean = torch.tensor([0.485, 0.456, 0.406]).to(device)\n",
    "cnn_normalization_std  = torch.tensor([0.229, 0.224, 0.225]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8bc07bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Normalization(nn.Module):\n",
    "    def __init__(self, mean, std):\n",
    "        super().__init__()\n",
    "        # reshape to [C x 1 x 1] so it can be broadcasted\n",
    "        self.mean = mean.clone().detach().view(-1,1,1)\n",
    "        self.std  = std.clone().detach().view(-1,1,1)\n",
    "    def forward(self, img):\n",
    "        return (img - self.mean) / self.std\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
